# -*- coding: utf-8 -*-
"""ScientificComputingWithPython_Exercise7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AxHaNHEUA1y38e5uWp1VVDGnJ1lx8z5f

# ***Exe.1:***
"""

!wget https://www.dropbox.com/s/aamg1apjhclecka/regression_generated.csv -P ./data/

import pandas as pd

df = pd.read_csv('./data/regression_generated.csv')

print(df)

#Feature_1 and Feature_2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
x = df.loc[: , 'features_1']
y = df.loc[: ,'features_2']
fig = plt.figure(figsize = (8, 8))
ax = fig.add_subplot()
ax.scatter(x , y , marker = 'o', c = 'b', edgecolor = 'w' , alpha = 0.5)
ax.set_xlabel("Features_1")
ax.set_ylabel("Features_2")

#Feature_1 and Feature_3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
x = df.loc[: , 'features_1']
y = df.loc[: , 'features_3']
fig = plt.figure(figsize = (8, 8))
ax = fig.add_subplot()
ax.scatter(x , y , marker = 'o', c = 'g', edgecolor = 'w' , alpha = 0.5)
ax.set_xlabel("Features_1")
ax.set_ylabel("Features_3")

#Feature_2 and Feature_3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
x = df.loc[: , 'features_2']
y = df.loc[: , 'features_3']
fig = plt.figure(figsize = (8, 8))
ax = fig.add_subplot()
ax.scatter(x , y , marker = 'o', c = 'r', edgecolor = 'w' , alpha = 0.5)
ax.set_xlabel("Features_2")
ax.set_ylabel("Features_3")

"""**Correlation is a statistical method used to determine if there is a connection or a relationship between two sets of data.**

So, I think they are correlated.

# ***Exe.2:***
"""

import numpy as np
import matplotlib.pyplot as plt

#A function for 2D.dataset with 2 categories.
def Guassian(Mean,StandardDeviation) : 
    return np.random.normal(Mean,StandardDeviation,1000)
x1 = Guassian(2,0.2)
y1 = Guassian(2,0.2)
plt.scatter(x1,y1)
x2 = Guassian(1,0.1)
y2 = Guassian(1,0.1)
plt.scatter(x2,y2)
plt.show()
#In continue I just try this function with different mean and standard deviation

x1 = Guassian(2,0.4)
y1 = Guassian(2,0.4)
plt.scatter(x1,y1)
x2 = Guassian(1,0.4)
y2 = Guassian(1,0.4)
plt.scatter(x2,y2)
plt.show()

x1 = Guassian(2,0.4)
y1 = Guassian(2,0.4)
plt.scatter(x1,y1)
x2 = Guassian(1,0.5)
y2 = Guassian(1,0.5)
plt.scatter(x2,y2)
plt.show()

"""# ***Exe.3:***"""

!wget https://www.dropbox.com/s/3uqleyc3wyz52tr/residuals_261.pkl -P data/

import numpy as np
import pandas as pd
Data = np.load("data/residuals_261.pkl",allow_pickle=True).item()
#Convert to dataframe
df = pd.DataFrame(Data)
df.head()   #We see that we have 2 variables.

#Inspecting the dataset
print("The names of the colomns are: ")
for Columns in df.columns:
    print(Columns)
print("")    
df.info()

#Cleaning the sample
df.drop(df[np.absolute(df['residuals']) < 2 ].index , inplace=True) 
df.info()

#Plot
import seaborn as sns
x = df.residuals
y = df.distances
sns.jointplot(x = "residuals", y = "distances", data = df)

#Linear regression
from scipy import stats
slope , intercept , r_value , p_value , stderr = stats.linregress(x, y)
sns.jointplot(x = "residuals", y = "distances", kind = "reg" , data = df)

#Bins for the histogram
Distances = np.array(df.distances)
Bins = [0,5,10,15,20,25]
DistanceBin = np.histogram(Distances , bins = Bins ) 
BinData =pd.cut(Distances,Bins)
Mean = df.groupby(pd.cut(df['distances'], bins = Bins))['residuals'].agg(['mean','std'])
print(Mean) #Mean for 5 ranges

#Profile histogram of the "distance" variable
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (12,6))
ax = fig.add_axes([0.1 , 0.1, 0.5, 0.5])
HistogramX = fig.add_axes([0.1 , 0.65 , 0.5 , 0.5], sharex = ax)
ax.scatter(df['distances'] , df['residuals'])
h , bins , _ = HistogramX.hist(df['distances'] , bins = 20)
x = 0.5 * (bins[1:] + bins[:-1])
y = np.zeros(len(bins))
err_y = np.zeros(len(bins))
for i in range(0 , len(x)-1):
    Mask = (df['distances'] > x[i]) & (df['distances'] < x[i+1])
    y[i] = np.mean(df[Mask].residuals)
    err_y[i] = np.std(df[Mask].residuals)

"""# ***Exe.4:***"""

#Easy way of this exercise if we just need the answer!
#Using the KernelDensity which is already prepared in sklearn library.
#With the Gaussian kernel

from sklearn.neighbors import KernelDensity
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
KDE = KernelDensity(kernel = 'gaussian', bandwidth = 0.2).fit(X)
KDE.score_samples(X)

#Solving the exercise step by step
#Fill a numpy array x of length N with a variable normally distributed, with a given mean and standard deviation
import numpy as np
N , Mean , STD = 1000 , 0 , 1
x =  np.random.normal(Mean , STD , size = N)
print(x)

# Commented out IPython magic to ensure Python compatibility.
#Fill an histogram
import matplotlib.pyplot as plt
# %matplotlib inline
import math
from matplotlib.ticker import MaxNLocator
fig, ((ax1, ax2)) = plt.subplots(nrows = 2, ncols = 1, figsize = (12,8))
Bins = int(math.sqrt(N))
Entries, edges, _ = ax1.hist(x, bins = Bins, color = 'c' , alpha = 0.5) 
ax1.yaxis.set_major_locator(MaxNLocator(integer = True))
ax1.set_title("Histogram for Normal Distribution: ")
ax2.hist(x, bins = Bins, color = 'r' , alpha = 0.5) 
ax2.errorbar(0.5 * (edges[:-1] + edges[1:]) , Entries, np.sqrt(Entries) , fmt = 'k.', markersize = 3, capsize = 3)
ax2.set_title("Histogram for Normal Distribution with Errors:")