# -*- coding: utf-8 -*-
"""ScientificExe08.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JJnghOeOjg8fvNQvaoND003Q9x6fmyDX

# ***Import Simple Essential libraries:***
"""

# Commented out IPython magic to ensure Python compatibility.
import math
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""# ***Exe01:***
**PCA on 3D dataset**

* Generate a dataset simulating 3 features, each with N entries (N being ${\cal O}(1000)$). Each feature is made by random numbers generated according the normal distribution $N(\mu,\sigma)$ with mean $\mu_i$ and standard deviation $\sigma_i$, with $i=1, 2, 3$. Generate the 3 variables $x_{i}$ such that:
    * $x_1$ is distributed as $N(0,1)$
    * $x_2$ is distributed as $x_1+N(0,3)$
    * $x_3$ is given by $2x_1+x_2$
* Find the eigenvectors and eigenvalues using the eigendecomposition of the covariance matrix
* Find the eigenvectors and eigenvalues using the SVD. Check that the two procedures yield to same result
* What percent of the total dataset's variability is explained by the principal components? Given how the dataset was constructed, do these make sense? Reduce the dimensionality of the system so that at least 99% of the total variability is retained
* Redefine the data according to the new basis from the PCA
* Plot the data, in both the original and the new basis. The figure should have 2 rows (the original and the new basis) and 3 columns (the $[x_0, x_1]$, $[x_0, x_2]$ and $[x_1, x_2]$ projections) of scatter plots.
"""

#Generate dataset
x1 = np.random.normal(0, 1, 1000)
x2 = x1 + np.random.normal(0, 3, 1000)
x3 = (2 * x1) + x2

#Convert to numpy array
Array = np.array([x1,x2,x3])

#Find covariance matrix
Covariance = np.cov(Array)
print("The Covariance is:\n " , Covariance)

#Distributions of data
sns.scatterplot(x1, x2, x3)

#Find the eigenvalues and vectors
#Using decomposition
from scipy import linalg as la
l, V = la.eig(Covariance)
print("The eigenvalues using the decomposition: \n" , l)
print("")
print("The real eigenvalues are: \n" , np.real_if_close(l))
print("")
print("The eigenvectors using the decomposition: \n" , V)

#Find the eigenvalues and vectors
#Using SVD
U, s, Vt = la.svd(Covariance)
print("U: \n" , U)
print("")
print("s: \n" , s)
print("")
print("Vt: \n" , Vt)

#Check if this two procedure have same results or not
print("The result of the two procedures are same: " , np.allclose(U , V))

#Check the original matrix
D = np.dot(V, np.dot(np.diag(np.real_if_close(l)), la.inv(V)))
print("Return the original matrix? " ,np.allclose(Covariance , D) )

Variability = np.real(np.sum(l[:2]) / np.sum(l)) * 100
print("The dataset variability is:" , Variability)
lSum = l.sum()
print("The variability of first element:" , np.real_if_close((l[0]/lSum) * 100))
print("The variability of second element:" , np.real_if_close((l[1]/lSum) * 100))
print("The variability of third element:" , np.real_if_close((l[2]/lSum) * 100))

scale_factor = 3
#Draw each eigenvectors rescaled by the eigenvalues
for li, vi in zip(l, V.T):
    print("Eigenvalue:", li, ",\teigenvector:", vi)
    #The line is defined by specifying its beginning and its end 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]], 'r-', lw=2)

#Rescale 
U, S, Vt = np.linalg.svd(Array)
scale_factor = 3
# Rescale the SVD spectrum to get the eigenvalues
l_svd = S ** 2 / (1000 - 1)
V_svd = U
print(V_svd)
print(l_svd)

Lambda = np.diag(l)
#Define the first axes for PCA
print(np.real_if_close(Lambda[0][0]/Covariance.trace()))

#First and Second and third axis
print(np.real_if_close(Lambda[1][1] + Lambda[0][0] + Lambda[2][2]))

la = np.diag(s)
print(la[0][0] + la[1][1] + la[2][2] , la.trace())

ax = plt.axes(projection = "3d")
plt.rcParams["figure.figsize"] = (10,10)
scale_factor = 3
ax.scatter(x1, x2, x3, linewidth = 0.01 , s = 100 , color = "r");
for li, vi in zip(l,V.T):
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]] , [0, scale_factor * li * vi[2]], "g", lw = 2)

New = np.dot(V.T, Array)
scale_factor = 6
ax = plt.axes(projection = "3d")
plt.rcParams["figure.figsize"] = (10,10)
ax.scatter(New[0,:], New[1,:] , New[2,:] , linewidth = 0.01 , s = 100 , color = "r")
#Rotate
for li, vi in zip(l_svd,V_svd.T): 
    plt.plot([0, scale_factor * li * vi[0]], [0, scale_factor * li * vi[1]],[0, scale_factor * li * vi[2]], "g", lw = 2)

fig, axes = plt.subplots(2, 3, figsize = (15,15))
axes[0,0].scatter(Array[0] , Array[1] , alpha = 0.4 , color = "g")
axes[0,1].scatter(Array[0] , Array[2] , alpha = 0.4 , color = "r")
axes[0,2].scatter(Array[1] , Array[2] , alpha = 0.4 , color = "b")
axes[1,0].scatter(New[0] , New[1] , alpha = 0.4 , color = "g")
axes[1,1].scatter(New[0] , New[2] , alpha = 0.4 , color = "r")
axes[1,2].scatter(New[1] , New[2] , alpha = 0.4 , color = "b")

"""# ***Exe02:***
**PCA on a nD dataset**

* Start from the dataset you have genereted in the previous exercise and add uncorrelated random noise. Such noise should be represented by other 10 uncorrelated variables normally distributed, with a standard deviation much smaller (e.g. a factor 20) than those used to generate the $x_1$ and $x_2$. Repeat the PCA procedure and compare the results with what you have obtained before.
"""

Noises = []
for i in range(0,10):
  Noises.append(np.random.normal(loc = 0, scale = 1/20, size = 1000))
Noisy = Array + np.sum(Noises)
print(Noisy)
print(Noisy.shape)

Covariance1 = np.cov(Noisy)
l1, V1 = la.eig(Covariance1)
print("The eigenvectors:\n " , V1)
print("")
print("The eigenvalues: " , l1)
print("")
print("The real eigenvalues :" , np.real_if_close(l1))

U1, S1, Vt1 = np.linalg.svd(Noisy)
scale_factor = 3
l_svd1 = S1 ** 2 / (1000 - 1)
V_svd1 = U1
print("The eigenvectors:\n " , V_svd1)
print("")
print("The eigenvalues: \n " ,l_svd1)

#Compare the result of original array with noisy one
NewNoisy = np.dot(V1.T, Noisy)
fig, axes = plt.subplots(2, 3, figsize = (15 , 15))
axes[0,0].scatter(Array[0] , Array[1] , alpha = 0.4 , color = "g")
axes[0,1].scatter(Array[0] , Array[2] , alpha = 0.4 , color = "r")
axes[0,2].scatter(Array[1] , Array[2] , alpha = 0.4 , color = "b")
axes[1,0].scatter(NewNoisy[0] , NewNoisy[1] , alpha = 0.4 , color = "g")
axes[1,1].scatter(NewNoisy[0] , NewNoisy[2] , alpha = 0.4 , color = "r")
axes[1,2].scatter(NewNoisy[1] , NewNoisy[2] , alpha = 0.4 , color = "b")

"""# ***Exe03:***
**Optional**: **PCA on the MAGIC dataset**

Perform a PCA on the magic04.data dataset.
"""

!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P data/
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P data/

df = pd.read_csv("data/magic04.data",header = None)
df

#The last column (column 10) is not we need in our calculation
DataSet = df.iloc[:,:10]
covariance = DataSet.cov()
print(covariance)

l, v = la.eig(covariance)
l = np.real_if_close(l)
xp = np.dot(v.T,DataSet.T)

fig, axs = plt.subplots(2,3,figsize=(32,15))
axs[0][0].set_title("x0 vs x1")
axs[0][0].scatter(DataSet.iloc[:,0],DataSet.iloc[:,1] , alpha = 0.5 ,  color = "g")
axs[0][1].set_title("x0 vs x2")
axs[0][1].scatter(DataSet.iloc[:,0],DataSet.iloc[:,2] , alpha = 0.5 , color = "r")
axs[0][2].set_title("x1 vs x2")
axs[0][2].scatter(DataSet.iloc[:,1],DataSet.iloc[:,2] , alpha = 0.5 , color = "b")
axs[1][0].set_title("xp0 vs xp1")
axs[1][0].scatter(xp[0,:],xp[1,:] , alpha = 0.5 , color = "c")
axs[1][1].set_title("xp0 vs xp2")
axs[1][1].scatter(xp[0,:],xp[2,:] , alpha = 0.5 , color = "purple")
axs[1][2].set_title("xp1 vs xp2")
axs[1][2].scatter(xp[1,:],xp[2,:] , alpha = 0.5 , color = "orange")